{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9-Z82i495Gl"
   },
   "source": [
    "#       Seq2Seq: Text Summarization with Keras\n",
    "#### ディープラーニングによる文章要約\n",
    "![](http://abigailsee.com/img/pointer-gen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw_oyfrE95Go"
   },
   "source": [
    "## Process\n",
    "1. Preprocessing\n",
    "2. Word2vec\n",
    "3. Building Seq2Seq Architecture\n",
    "4. Training with  BBC article&summary Dataset\n",
    "5. Generate Summary with my_summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2QnkkQv95Gq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shantanu\\\\Dropbox\\\\NLP-Workshop\\\\Text_Summarization_using_Bidirectional_LSTM-master'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeKq5tai95Gu"
   },
   "outputs": [],
   "source": [
    "news_category = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "row_doc = \"data/News Articles/\"\n",
    "summary_doc = \"data/Summaries/\"\n",
    "\n",
    "data={\"articles\":[], \"summaries\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "682uVCoC95Gy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directories = {\"news\": row_doc, \"summary\": summary_doc}\n",
    "row_dict = {}\n",
    "sum_dict = {}\n",
    "\n",
    "for path in directories.values():\n",
    "    if path == row_doc:\n",
    "        file_dict = row_dict\n",
    "    else:\n",
    "        file_dict = sum_dict\n",
    "    dire = path\n",
    "    for cat in news_category:\n",
    "        category = cat\n",
    "        files = os.listdir(dire + category)\n",
    "        file_dict[cat] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM75fs9g95Hc"
   },
   "outputs": [],
   "source": [
    "row_data = {}\n",
    "for cat in row_dict.keys():\n",
    "    cat_dict = {}\n",
    "    # row_data_frame[cat] = []\n",
    "    for i in range(0, len(row_dict[cat])):\n",
    "        filename = row_dict[cat][i]\n",
    "        path = row_doc + cat + \"/\" + filename\n",
    "        with open(path, \"rb\") as f:                \n",
    "            text = f.read()\n",
    "            cat_dict[filename[:3]] = text\n",
    "    row_data[cat] = cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjPzSMPT95Hg"
   },
   "outputs": [],
   "source": [
    "sum_data = {}\n",
    "for cat in sum_dict.keys():\n",
    "    cat_dict = {}\n",
    "    # row_data_frame[cat] = []\n",
    "    for i in range(0, len(sum_dict[cat])):\n",
    "        filename = sum_dict[cat][i]\n",
    "        path = summary_doc + cat + \"/\" + filename\n",
    "        with open(path, \"rb\") as f:                \n",
    "            text = f.read()\n",
    "            cat_dict[filename[:3]] = text\n",
    "    sum_data[cat] = cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "du70GJ7o95H0",
    "outputId": "2f05e596-129f-489e-8fcd-e789f870fe9e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>b'Ad sales boost Time Warner profit\\n\\nQuarter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002</th>\n",
       "      <td>b'Dollar gains on Greenspan speech\\n\\nThe doll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003</th>\n",
       "      <td>b'Yukos unit buyer faces loan claim\\n\\nThe own...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004</th>\n",
       "      <td>b'High fuel prices hit BA\\'s profits\\n\\nBritis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005</th>\n",
       "      <td>b\"Pernod takeover talk lifts Domecq\\n\\nShares ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           row_article\n",
       "001  b'Ad sales boost Time Warner profit\\n\\nQuarter...\n",
       "002  b'Dollar gains on Greenspan speech\\n\\nThe doll...\n",
       "003  b'Yukos unit buyer faces loan claim\\n\\nThe own...\n",
       "004  b'High fuel prices hit BA\\'s profits\\n\\nBritis...\n",
       "005  b\"Pernod takeover talk lifts Domecq\\n\\nShares ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_business = pd.DataFrame.from_dict(row_data[\"business\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKbrhIYw95H8"
   },
   "outputs": [],
   "source": [
    "#  news_category = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "news_entertainment = pd.DataFrame.from_dict(row_data[\"entertainment\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_politics = pd.DataFrame.from_dict(row_data[\"politics\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_sport = pd.DataFrame.from_dict(row_data[\"sport\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_tech = pd.DataFrame.from_dict(row_data[\"tech\"], orient=\"index\", columns=[\"row_article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrSmXmON95IE"
   },
   "outputs": [],
   "source": [
    "# summary data\n",
    "summary_business = pd.DataFrame.from_dict(sum_data[\"business\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_entertainment = pd.DataFrame.from_dict(sum_data[\"entertainment\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_politics = pd.DataFrame.from_dict(sum_data[\"politics\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_sport = pd.DataFrame.from_dict(sum_data[\"sport\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_tech = pd.DataFrame.from_dict(sum_data[\"tech\"], orient=\"index\", columns=[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNL3-2xL95IJ",
    "outputId": "8dd40f58-2921-4c73-fd62-6b5b9e673169"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>b\"TimeWarner said fourth quarter sales rose 2%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002</th>\n",
       "      <td>b'The dollar has hit its highest level against...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003</th>\n",
       "      <td>b'Yukos\\' owner Menatep Group says it will ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004</th>\n",
       "      <td>b'Rod Eddington, BA\\'s chief executive, said t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005</th>\n",
       "      <td>b\"Pernod has reduced the debt it took on to fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               summary\n",
       "001  b\"TimeWarner said fourth quarter sales rose 2%...\n",
       "002  b'The dollar has hit its highest level against...\n",
       "003  b'Yukos\\' owner Menatep Group says it will ask...\n",
       "004  b'Rod Eddington, BA\\'s chief executive, said t...\n",
       "005  b\"Pernod has reduced the debt it took on to fu..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aq407qkG95IO"
   },
   "outputs": [],
   "source": [
    "business = news_business.join(summary_business, how='inner')\n",
    "entertainment = news_entertainment.join(summary_entertainment, how='inner')\n",
    "politics = news_politics.join(summary_politics, how='inner')\n",
    "sport = news_sport.join(summary_sport, how='inner')\n",
    "tech = news_tech.join(summary_tech, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKbjkn-r95IR"
   },
   "outputs": [],
   "source": [
    "business = news_business.join(summary_business, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKfHtKbL95IW",
    "outputId": "11b33d28-b909-4924-8e97-3e978c473fe8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>b'Ad sales boost Time Warner profit\\n\\nQuarter...</td>\n",
       "      <td>b\"TimeWarner said fourth quarter sales rose 2%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002</th>\n",
       "      <td>b'Dollar gains on Greenspan speech\\n\\nThe doll...</td>\n",
       "      <td>b'The dollar has hit its highest level against...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003</th>\n",
       "      <td>b'Yukos unit buyer faces loan claim\\n\\nThe own...</td>\n",
       "      <td>b'Yukos\\' owner Menatep Group says it will ask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004</th>\n",
       "      <td>b'High fuel prices hit BA\\'s profits\\n\\nBritis...</td>\n",
       "      <td>b'Rod Eddington, BA\\'s chief executive, said t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005</th>\n",
       "      <td>b\"Pernod takeover talk lifts Domecq\\n\\nShares ...</td>\n",
       "      <td>b\"Pernod has reduced the debt it took on to fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           row_article  \\\n",
       "001  b'Ad sales boost Time Warner profit\\n\\nQuarter...   \n",
       "002  b'Dollar gains on Greenspan speech\\n\\nThe doll...   \n",
       "003  b'Yukos unit buyer faces loan claim\\n\\nThe own...   \n",
       "004  b'High fuel prices hit BA\\'s profits\\n\\nBritis...   \n",
       "005  b\"Pernod takeover talk lifts Domecq\\n\\nShares ...   \n",
       "\n",
       "                                               summary  \n",
       "001  b\"TimeWarner said fourth quarter sales rose 2%...  \n",
       "002  b'The dollar has hit its highest level against...  \n",
       "003  b'Yukos\\' owner Menatep Group says it will ask...  \n",
       "004  b'Rod Eddington, BA\\'s chief executive, said t...  \n",
       "005  b\"Pernod has reduced the debt it took on to fu...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sUHqLUJ95Ib",
    "outputId": "7b492210-e0b1-4622-fcba-9c0f730ca3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 2560\n",
      "sum 901\n"
     ]
    }
   ],
   "source": [
    "print(\"row\", len(business.iloc[0,0]))\n",
    "print(\"sum\", len(business.iloc[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VthiJz-h95Ij"
   },
   "outputs": [],
   "source": [
    "list_df = [business, entertainment, politics, sport, tech]\n",
    "length = 0\n",
    "for df in list_df:\n",
    "    length += len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hj44ZFO-95Iu",
    "outputId": "f5649126-2cf2-45ad-cc77-879985450f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all data:  2225\n"
     ]
    }
   ],
   "source": [
    "print(\"length of all data: \", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFtKW1DS95I2",
    "outputId": "f9e46aae-aa16-4415-8302-ddd47bd220ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_df = pd.concat([business, entertainment, politics, sport, tech], ignore_index=True)\n",
    "len(bbc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xogCv95T95JF"
   },
   "source": [
    "## Step 2. Preprocessing Text Data\n",
    "1. Clean Text\n",
    "2. Tokenize\n",
    "3. Vocabrary\n",
    "4. Padding\n",
    "5. One-Hot Encoding\n",
    "6. Reshape to (MAX_LEN, One-Hot Encoding DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y2un_KkA95JU"
   },
   "source": [
    "### 2-1. Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxQY2wvq95JJ"
   },
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text=text.split()\n",
    "    words=[]\n",
    "    for t in text:\n",
    "        if t.isalpha():\n",
    "            words.append(t)\n",
    "    text=\" \".join(words)\n",
    "    text=text.lower()\n",
    "    text=re.sub(r\"what's\",\"what is \",text)\n",
    "    text=re.sub(r\"it's\",\"it is \",text)\n",
    "    text=re.sub(r\"\\'ve\",\" have \",text)\n",
    "    text=re.sub(r\"i'm\",\"i am \",text)\n",
    "    text=re.sub(r\"\\'re\",\" are \",text)\n",
    "    text=re.sub(r\"n't\",\" not \",text)\n",
    "    text=re.sub(r\"\\'d\",\" would \",text)\n",
    "    text=re.sub(r\"\\'s\",\"s\",text)\n",
    "    text=re.sub(r\"\\'ll\",\" will \",text)\n",
    "    text=re.sub(r\"can't\",\" cannot \",text)\n",
    "    text=re.sub(r\" e g \",\" eg \",text)\n",
    "    text=re.sub(r\"e-mail\",\"email\",text)\n",
    "    text=re.sub(r\"9\\\\/11\",\" 911 \",text)\n",
    "    text=re.sub(r\" u.s\",\" american \",text)\n",
    "    text=re.sub(r\" u.n\",\" united nations \",text)\n",
    "    text=re.sub(r\"\\n\",\" \",text)\n",
    "    text=re.sub(r\":\",\" \",text)\n",
    "    text=re.sub(r\"-\",\" \",text)\n",
    "    text=re.sub(r\"\\_\",\" \",text)\n",
    "    text=re.sub(r\"\\d+\",\" \",text)\n",
    "    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7ovvCt495JL"
   },
   "outputs": [],
   "source": [
    "for col in bbc_df.columns:\n",
    "    bbc_df[col] = bbc_df[col].apply(lambda x: cleantext(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwbxBACO95JP",
    "outputId": "8b9cf24a-1714-47bf-944b-4aaa54ab78c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sales boost time warner profits at us media gi...</td>\n",
       "      <td>said fourth quarter sales rose to from the tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gains on greenspan dollar has hit its highest ...</td>\n",
       "      <td>dollar has hit its highest level against the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unit buyer faces loan owners of embattled russ...</td>\n",
       "      <td>owner menatep group says it will ask rosneft t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fuel prices hit airways has blamed high fuel p...</td>\n",
       "      <td>chief said the results were in a third quarter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>takeover talk lifts in uk drinks and food firm...</td>\n",
       "      <td>has reduced the debt it took on to fund the se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         row_article  \\\n",
       "0  sales boost time warner profits at us media gi...   \n",
       "1  gains on greenspan dollar has hit its highest ...   \n",
       "2  unit buyer faces loan owners of embattled russ...   \n",
       "3  fuel prices hit airways has blamed high fuel p...   \n",
       "4  takeover talk lifts in uk drinks and food firm...   \n",
       "\n",
       "                                             summary  \n",
       "0  said fourth quarter sales rose to from the tim...  \n",
       "1  dollar has hit its highest level against the e...  \n",
       "2  owner menatep group says it will ask rosneft t...  \n",
       "3  chief said the results were in a third quarter...  \n",
       "4  has reduced the debt it took on to fund the se...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "JBPbkd8pIORs",
    "outputId": "51066e27-b9c1-467e-845a-b162e813ce2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>001</th>\n",
       "      <td>b'Ink helps drive democracy in Asia\\n\\nThe Kyr...</td>\n",
       "      <td>b'The other common type of ink in elections is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002</th>\n",
       "      <td>b'China net cafe culture crackdown\\n\\nChinese ...</td>\n",
       "      <td>b\"Laws on net cafe opening hours and who can u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003</th>\n",
       "      <td>b\"Microsoft seeking spyware trojan\\n\\nMicrosof...</td>\n",
       "      <td>b\"Microsoft is investigating a trojan program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004</th>\n",
       "      <td>b'Digital guru floats sub-$100 PC\\n\\nNicholas ...</td>\n",
       "      <td>b'He said one laptop per child could be \" very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005</th>\n",
       "      <td>b'Technology gets the creative bug\\n\\nThe hi-t...</td>\n",
       "      <td>b'\"We are hoping to understand the creative in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           row_article  \\\n",
       "001  b'Ink helps drive democracy in Asia\\n\\nThe Kyr...   \n",
       "002  b'China net cafe culture crackdown\\n\\nChinese ...   \n",
       "003  b\"Microsoft seeking spyware trojan\\n\\nMicrosof...   \n",
       "004  b'Digital guru floats sub-$100 PC\\n\\nNicholas ...   \n",
       "005  b'Technology gets the creative bug\\n\\nThe hi-t...   \n",
       "\n",
       "                                               summary  \n",
       "001  b'The other common type of ink in elections is...  \n",
       "002  b\"Laws on net cafe opening hours and who can u...  \n",
       "003  b\"Microsoft is investigating a trojan program ...  \n",
       "004  b'He said one laptop per child could be \" very...  \n",
       "005  b'\"We are hoping to understand the creative in...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YA9tvTPUIciB",
    "outputId": "1780794c-4748-49ba-8669-c75b875347d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2969"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list =[]\n",
    "for article in df.row_article:\n",
    "    words = article.split()\n",
    "    length = len(words)\n",
    "    len_list.append(length)\n",
    "max(len_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jO14nm8JJPzZ"
   },
   "source": [
    "### 2-2. Tokenizer\n",
    "1. Tokenize and One-Hot : Tokenizer\n",
    "2. Vocabraly: article and summary 15000 words \n",
    "3. Padding: pad_sequences 1000 max_len\n",
    "4. Reshape: manual max_len * one-hot matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>continues rapid economy has expanded by a brea...</td>\n",
       "      <td>overall investment in fixed assets was still u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deccan seals deccan has ordered airbus planes ...</td>\n",
       "      <td>government has given its backing to cheaper an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>job growth continues in us created fewer jobs ...</td>\n",
       "      <td>creation was one of last main concerns for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owner buys rival for retail giant federated de...</td>\n",
       "      <td>retail giant federated department stores is to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>secures giant japan is to supply japan airline...</td>\n",
       "      <td>chose the after carefully considering both it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         row_article  \\\n",
       "0  continues rapid economy has expanded by a brea...   \n",
       "1  deccan seals deccan has ordered airbus planes ...   \n",
       "2  job growth continues in us created fewer jobs ...   \n",
       "3  owner buys rival for retail giant federated de...   \n",
       "4  secures giant japan is to supply japan airline...   \n",
       "\n",
       "                                             summary  \n",
       "0  overall investment in fixed assets was still u...  \n",
       "1  government has given its backing to cheaper an...  \n",
       "2  creation was one of last main concerns for the...  \n",
       "3  retail giant federated department stores is to...  \n",
       "4  chose the after carefully considering both it ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_art_sum = pd.read_csv(\"cleaned_bbc_news.csv\")\n",
    "bbc_art_sum.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "bbc_art_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(bbc_art_sum.row_article)\n",
    "summaries = list(bbc_art_sum.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as tfk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-1. Tokenize: text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W9aALHY_YbUN",
    "outputId": "b228cf03-c36b-413a-d3c8-c9cb6fff1b36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23914"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "VOCAB_SIZE = 5000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(articles)\n",
    "article_sequences = tokenizer.texts_to_sequences(articles)\n",
    "art_word_index = tokenizer.word_index\n",
    "len(art_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1411, 2338, 248, 16, 3994, 22, 5, 165, 1359, 50, 966, 4, 120, 967, 176, 118, 505, 38, 2339, 10]\n",
      "[16, 2233, 3001, 3441, 6, 5, 217, 18, 60, 1270, 6, 1, 827, 11, 108, 45, 59, 86, 4, 470]\n",
      "[478, 196, 1411, 6, 54, 736, 2283, 498, 50, 164, 6, 24, 349, 17, 9, 1, 3322, 6, 11, 26]\n"
     ]
    }
   ],
   "source": [
    "print(article_sequences[0][:20])\n",
    "print(article_sequences[1][:20])\n",
    "print(article_sequences[2][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-2. Vocabraly: article and summary 15000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_word_index_1500 = {}\n",
    "counter = 0\n",
    "for word in art_word_index.keys():\n",
    "    if art_word_index[word] == 0:\n",
    "        print(\"found 0!\")\n",
    "        break\n",
    "    if art_word_index[word] > VOCAB_SIZE:\n",
    "        continue\n",
    "    else:\n",
    "        art_word_index_1500[word] = art_word_index[word]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23929"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(summaries)\n",
    "summary_sequences = tokenizer.texts_to_sequences(summaries)\n",
    "sum_word_index = tokenizer.word_index\n",
    "len(sum_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_index_1500 = {}\n",
    "counter = 0\n",
    "for word in sum_word_index.keys():\n",
    "    if sum_word_index[word] == 0:\n",
    "        print(\"found 0!\")\n",
    "        break\n",
    "    if sum_word_index[word] > VOCAB_SIZE:\n",
    "        continue\n",
    "    else:\n",
    "        sum_word_index_1500[word] = sum_word_index[word]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-3. Padding: pad_sequences 1000 max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 100\n",
    "pad_art_sequences = pad_sequences(article_sequences, maxlen=MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 100\n"
     ]
    }
   ],
   "source": [
    "print(len(article_sequences[1]), len(pad_art_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sum_sequences = pad_sequences(summary_sequences, maxlen=MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 100\n"
     ]
    }
   ],
   "source": [
    "print(len(summary_sequences[1]), len(pad_sum_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_art_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1411, 2338,  248, ...,    2, 1110,    9],\n",
       "       [  16, 2233, 3001, ...,    4,   38, 3710],\n",
       "       [ 478,  196, 1411, ...,    1, 4192,   24],\n",
       "       ...,\n",
       "       [ 421, 1337, 2012, ...,    5,    3,    5],\n",
       "       [2164,  267, 1109, ...,   32,   34,  441],\n",
       "       [   7,  284,    8, ...,    4,  753, 1528]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_art_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-4. Reshape: manual max_len * one-hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nencoder_inputs = np.zeros((2225, 1000), dtype='float32')\\nencoder_inputs.shape\\n\\ndecoder_inputs = np.zeros((2225, 1000), dtype='float32')\\ndecoder_inputs.shape\\n\\nfor i, seqs in enumerate(pad_art_sequences):\\n    for j, seq in enumerate(seqs):\\n        encoder_inputs[i, j] = seq\\n        \\nfor i, seqs in enumerate(pad_sum_sequences):\\n    for j, seq in enumerate(seqs):\\n        decoder_inputs[i, j] = seq\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使わない\n",
    "\"\"\"\n",
    "encoder_inputs = np.zeros((2225, 1000), dtype='float32')\n",
    "encoder_inputs.shape\n",
    "\n",
    "decoder_inputs = np.zeros((2225, 1000), dtype='float32')\n",
    "decoder_inputs.shape\n",
    "\n",
    "for i, seqs in enumerate(pad_art_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        encoder_inputs[i, j] = seq\n",
    "        \n",
    "for i, seqs in enumerate(pad_sum_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        decoder_inputs[i, j] = seq\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_outputs = np.zeros((2225, 1000, 15000), dtype='float32')\n",
    "# decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, seqs in enumerate(pad_sum_sequences):\n",
    "#     for j, seq in enumerate(seqs):\n",
    "#         decoder_outputs[i, j, seq] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-5. Pre-trained word2vec and word2vec Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sokvMj0Xfnjw",
    "outputId": "3ef40349-e3e7-4f25-d263-7d316d21f829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdx3r2rvZAwN"
   },
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(embedding_dimention, word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VB6UmXeqWleO",
    "outputId": "59e76cc0-91a2-454b-a61e-f7cb1b2cf59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# art_embedding_matrix = embedding_matrix_creater(200, word_index=art_word_index_1500)\n",
    "art_embedding_matrix = embedding_matrix_creater(50, word_index=art_word_index_1500)\n",
    "art_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1KgrdRFvb7A1",
    "outputId": "e01036f5-476b-46eb-bec7-1ad263ba9dd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 50)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embedding_matrix = embedding_matrix_creater(50, word_index=sum_word_index_1500)\n",
    "sum_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuawT3rXUIBY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_layer = Embedding(input_dim = 5001, \n",
    "                                    output_dim = 50,\n",
    "                                    input_length = MAX_LEN,\n",
    "                                    weights = [art_embedding_matrix],\n",
    "                                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf3EzUZcIccZ"
   },
   "outputs": [],
   "source": [
    "decoder_embedding_layer = Embedding(input_dim = 5001, \n",
    "                                    output_dim = 50,\n",
    "                                    input_length = MAX_LEN,\n",
    "                                    weights = [sum_embedding_matrix],\n",
    "                                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J5acWwCxIcZz",
    "outputId": "a92cd279-b2de-4110-fd1d-35e13e90c099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AB663AaY0PKH"
   },
   "source": [
    "## Step 3. Building Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-kZT0pQRqRl"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "# import chart_studio.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import pydot\n",
    "\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras import backend as k\n",
    "k.set_learning_phase(1)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,LSTM,Dropout,Input,Activation,Add,concatenate, Embedding, RepeatVector\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psWPxabWIcWz"
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "MAX_LEN = 100\n",
    "VOCAB_SIZE =5000\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_UNITS = 200\n",
    "VOCAB_SIZE = VOCAB_SIZE + 1\n",
    "\n",
    "LEARNING_RATE = 0.002\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing Data Split\n",
    "import numpy as np\n",
    "num_samples = len(pad_sum_sequences)\n",
    "decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"int32\")\n",
    "\n",
    "# outputの３Dテンソル\n",
    "for i, seqs in enumerate(pad_sum_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        if j > 0:\n",
    "            decoder_output_data[i][j][seq] = 1\n",
    "            \n",
    "art_train, art_test, sum_train, sum_test = train_test_split(pad_art_sequences, pad_sum_sequences, test_size=0.2)\n",
    "\n",
    "train_num = art_train.shape[0]\n",
    "train_num\n",
    "\n",
    "target_train = decoder_output_data[:train_num]\n",
    "target_test = decoder_output_data[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. Simple LSTM Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGjWMUG3IcT_"
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Simple LSTM Encoder-Decoder-seq2seq\n",
    "# \"\"\"\n",
    "# # encoder\n",
    "# encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "# encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "# encoder_LSTM = LSTM(HIDDEN_UNITS)(encoder_embedding)\n",
    "# # decoder\n",
    "# decoder_inputs = Input(shape=(MAX_LEN, ))\n",
    "# decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "# decoder_LSTM = LSTM(200)(decoder_embedding)\n",
    "# # merge\n",
    "# merge_layer = concatenate([encoder_LSTM, decoder_LSTM])\n",
    "# decoder_outputs = Dense(units=VOCAB_SIZE+1, activation=\"softmax\")(merge_layer) # SUM_VOCAB_SIZE, sum_embedding_matrix.shape[1]\n",
    "\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHw0flwp4uW2"
   },
   "outputs": [],
   "source": [
    "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# history = model.fit([art_train, sum_train], \n",
    "#                      target_train, \n",
    "#                      epochs=EPOCHS, \n",
    "#                      batch_size=BATCH_SIZE,\n",
    "#                      validation_data=([art_test, sum_test], target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2. Bidirectional LSTM Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bidirectional LSTM: Others Inspired Encoder-Decoder-seq2seq\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(MAX_LEN,))\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_LSTM_R = LSTM(HIDDEN_UNITS, return_state=True, go_backwards=True)\n",
    "encoder_outputs_R, state_h_R, state_c_R = encoder_LSTM_R(encoder_embedding)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "final_h = Add()([state_h, state_h_R])\n",
    "final_c = Add()([state_c, state_c_R])\n",
    "encoder_states = [final_h, final_c]\n",
    "\n",
    "\"\"\"\n",
    "decoder\n",
    "\"\"\"\n",
    "decoder_inputs = Input(shape=(MAX_LEN,))\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states) \n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='linear')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Shantanu\\Anaconda3\\envs\\TXTSUMG\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/5\n",
      "1780/1780 [==============================] - 28s 16ms/sample - loss: 9.8539 - acc: 0.0745 - val_loss: 8.2601 - val_acc: 0.0182\n",
      "Epoch 2/5\n",
      "1780/1780 [==============================] - 26s 14ms/sample - loss: 9.8004 - acc: 0.0219 - val_loss: 9.6269 - val_acc: 0.0105\n",
      "Epoch 3/5\n",
      "1780/1780 [==============================] - 26s 14ms/sample - loss: 10.6502 - acc: 0.0040 - val_loss: 10.5547 - val_acc: 0.0061\n",
      "Epoch 4/5\n",
      "1780/1780 [==============================] - 25s 14ms/sample - loss: 8.6880 - acc: 0.0196 - val_loss: 13.7647 - val_acc: 0.0236\n",
      "Epoch 5/5\n",
      "1780/1780 [==============================] - 25s 14ms/sample - loss: 13.9106 - acc: 0.0214 - val_loss: 13.2153 - val_acc: 0.0254\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit([art_train, sum_train], \n",
    "                     target_train, \n",
    "                     epochs=EPOCHS, \n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     validation_data=([art_test, sum_test], target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3. Chatbot Inspired Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Chatbot Inspired Encoder-Decoder-seq2seq\n",
    "# \"\"\"\n",
    "# encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "# encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "# encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "# decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "# decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "# decoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\n",
    "# decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "# outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "# model = Model([encoder_inputs, decoder_inputs], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop = RMSprop(lr=0.01, clipnorm=1.)\n",
    "# model.compile(loss='mse', optimizer=rmsprop, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Training your model and Validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# num_samples = len(pad_sum_sequences)\n",
    "# decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputの３Dテンソル\n",
    "# for i, seqs in enumerate(pad_sum_sequences):\n",
    "#     for j, seq in enumerate(seqs):\n",
    "#         if j > 0:\n",
    "#             decoder_output_data[i][j][seq] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFxSEKvfIoGf"
   },
   "outputs": [],
   "source": [
    "# art_train, art_test, sum_train, sum_test = train_test_split(pad_art_sequences, pad_sum_sequences, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9hn_1hm-JCa2",
    "outputId": "95c79153-06ec-4b2d-b25e-6f34c933ae44"
   },
   "outputs": [],
   "source": [
    "# train_num = art_train.shape[0]\n",
    "# train_num\n",
    "\n",
    "# target_train = decoder_output_data[:train_num]\n",
    "# target_test = decoder_output_data[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_train = decoder_output_data[:train_num]\n",
    "# target_test = decoder_output_data[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit([art_train, sum_train], \n",
    "#                      target_train, \n",
    "#                      epochs=EPOCHS, \n",
    "#                      batch_size=BATCH_SIZE,\n",
    "#                      validation_data=([art_test, sum_test], target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 正確性の可視化\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 損失関数の可視化\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(history.history['loss'])\n",
    "# # plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# # plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # モデルの読み込み\n",
    "# # with open('text_summary.json',\"w\").write(model.to_json())\n",
    "\n",
    "# # 重みの読み込み\n",
    "# model.load_weights('text_summary.h5')\n",
    "# print(\"Saved Model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "input_matrix.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:TXTSUMG] *",
   "language": "python",
   "name": "conda-env-TXTSUMG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
